import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt 
from sklearn.model_selection import train_test_split
torch.manual_seed(42)

# Generate the data
x = torch.randn(200, 1)  # 200 sample points, 1 feature
y = x**2 + 2*x + 3 + 0.1 * torch.randn(200, 1)

# Split the data into training and testing sets (70% training, 30% testing)
x_train, x_test, y_train, y_test = train_test_split(x.numpy(), y.numpy(), test_size=0.3, random_state=42)

# Convert numpy arrays to torch tensors
x_train = torch.tensor(x_train, dtype=torch.float32)
x_test = torch.tensor(x_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

# Define the neural network model class
class Nonlinear(nn.Module):
    def __init__(self):
        super(Nonlinear, self).__init__()
        self.layer1 = nn.Linear(1, 10)  # Input layer with 1 feature and 10 units
        self.layer2 = nn.Linear(10, 1)  # Output layer with 10 units and 1 output
        self.relu = nn.ReLU()  # ReLU activation function

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)  # Apply ReLU activation after the first layer
        x = self.layer2(x)
        return x

# Instantiate the model
model = Nonlinear()

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)


# Training loop (example for illustration)
epochs = 1000
for epoch in range(epochs):
    model.train()
    
    # Forward pass
    predictions = model(x_train)
    loss = criterion(predictions, y_train)
    
    # Backward pass and optimization
    optimizer.zero_grad()  # Zero the gradients from the previous step
    loss.backward()  # Backpropagation
    optimizer.step()  # Update the model weights
    
    # Print the loss every 100 epochs for monitoring
    if epoch % 100 == 0:
        print(f"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}")

# Testing the model (example for illustration)
model.eval()  # Set the model to evaluation mode
with torch.no_grad():  # No need to track gradients during testing
    predictions_test = model(x_test)
    test_loss = criterion(predictions_test, y_test)
    print(f"Test Loss: {test_loss.item():.4f}")
